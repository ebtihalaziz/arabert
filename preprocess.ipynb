{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfJn9zVM9gnwS/nCbJB+0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebtihalaziz/arabert/blob/master/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq2H8Y-Jsfeb"
      },
      "source": [
        "!pip install PyArabic\n",
        "!pip install farasapy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2eQSBL2mzIq"
      },
      "source": [
        "import html\n",
        "import logging\n",
        "import re\n",
        "\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "ACCEPTED_MODELS = [\n",
        "    \"bert-base-arabertv01\",\n",
        "    \"bert-base-arabert\",\n",
        "    \"bert-base-arabertv02\",\n",
        "    \"bert-base-arabertv2\",\n",
        "    \"bert-large-arabertv02\",\n",
        "    \"bert-large-arabertv2\",\n",
        "    \"araelectra-base\",\n",
        "    \"araelectra-base-discriminator\",\n",
        "    \"araelectra-base-generator\",\n",
        "    \"aragpt2-base\",\n",
        "    \"aragpt2-medium\",\n",
        "    \"aragpt2-large\",\n",
        "    \"aragpt2-mega\",\n",
        "]\n",
        "\n",
        "SEGMENTED_MODELS = [\n",
        "    \"bert-base-arabert\",\n",
        "    \"bert-base-arabertv2\",\n",
        "    \"bert-large-arabertv2\",\n",
        "]\n",
        "\n",
        "\n",
        "class  ArbertmoPreprocessor:\n",
        "    \"\"\"\n",
        "    A Preprocessor class that cleans and preprocesses text for all models in the AraBERT repo.\n",
        "    It also can unprocess the text ouput of the generated text\n",
        "\n",
        "    Args:\n",
        "\n",
        "        model_name (:obj:`str`): model name from the HuggingFace Models page without the aubmindlab tag. Defaults to \"bert-base-arabertv02\". Current accepted models are:\n",
        "\n",
        "            - :obj:`\"bert-base-arabertv01\"`: No farasa segmentation.\n",
        "            - :obj:`\"bert-base-arabert\"`: with farasa segmentation.\n",
        "            - :obj:`\"bert-base-arabertv02\"`: No farasas egmentation.\n",
        "            - :obj:`\"bert-base-arabertv2\"`: with farasa segmentation.\n",
        "            - :obj:`\"bert-large-arabertv02\"`: No farasas egmentation.\n",
        "            - :obj:`\"bert-large-arabertv2\"`: with farasa segmentation.\n",
        "            - :obj:`\"araelectra-base\"`: No farasa segmentation.\n",
        "            - :obj:`\"araelectra-base-discriminator\"`: No farasa segmentation.\n",
        "            - :obj:`\"araelectra-base-generator\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-base\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-medium\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-large\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-mega\"`: No farasa segmentation.\n",
        "\n",
        "        keep_emojis(:obj: `bool`): don't remove emojis while preprocessing. Defaults to False\n",
        "\n",
        "        remove_html_markup(:obj: `bool`): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True\n",
        "\n",
        "        replace_urls_emails_mentions(:obj: `bool`): Whether to replace email urls and mentions by special tokens. Defaults to True\n",
        "\n",
        "        strip_tashkeel(:obj: `bool`): remove diacritics (FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SUKUN, SHADDA)\n",
        "\n",
        "        strip_tatweel(:obj: `bool`): remove tatweel '\\\\u0640'\n",
        "\n",
        "        insert_white_spaces(:obj: `bool`): insert whitespace before and after all non Arabic digits or English digits or Arabic and English Alphabet or the 2 brackets, then inserts whitespace between words and numbers or numbers and words\n",
        "\n",
        "        remove_elongation(:obj: `bool`): replace repetition of more than 2 non-digit character with 2 of this character\n",
        "\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        ArBERTMoPreprocessor: the preprocessor class\n",
        "\n",
        "    Example:\n",
        "\n",
        "        from preprocess import ArBERTMoPreprocessor\n",
        "\n",
        "        arabert_prep = ArBERTMoPreprocessor(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "        arabert_prep.preprocess(\"SOME ARABIC TEXT\")\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name,\n",
        "        keep_emojis=False,\n",
        "        remove_html_markup=True,\n",
        "        replace_urls_emails_mentions=True,\n",
        "        strip_tashkeel=True,\n",
        "        strip_tatweel=True,\n",
        "        insert_white_spaces=True,\n",
        "        remove_elongation=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        model_name (:obj:`str`): model name from the HuggingFace Models page without the aubmindlab tag. Defaults to \"bert-base-arabertv02\". Current accepted models are:\n",
        "\n",
        "            - :obj:`\"bert-base-arabertv01\"`: No farasa segmentation.\n",
        "            - :obj:`\"bert-base-arabert\"`: with farasa segmentation.\n",
        "            - :obj:`\"bert-base-arabertv02\"`: No farasas egmentation.\n",
        "            - :obj:`\"bert-base-arabertv2\"`: with farasa segmentation.\n",
        "            - :obj:`\"bert-large-arabertv02\"`: No farasas egmentation.\n",
        "            - :obj:`\"bert-large-arabertv2\"`: with farasa segmentation.\n",
        "            - :obj:`\"araelectra-base\"`: No farasa segmentation.\n",
        "            - :obj:`\"araelectra-base-discriminator\"`: No farasa segmentation.\n",
        "            - :obj:`\"araelectra-base-generator\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-base\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-medium\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-large\"`: No farasa segmentation.\n",
        "            - :obj:`\"aragpt2-mega\"`: No farasa segmentation.\n",
        "\n",
        "        keep_emojis(:obj: `bool`): don't remove emojis while preprocessing. Defaults to False\n",
        "\n",
        "        remove_html_markup(:obj: `bool`): Whether to remove html artfacts, should be set to False when preprocessing TyDi QA. Defaults to True\n",
        "\n",
        "        replace_urls_emails_mentions(:obj: `bool`): Whether to replace email urls and mentions by special tokens. Defaults to True\n",
        "\n",
        "        strip_tashkeel(:obj: `bool`): remove diacritics (FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SUKUN, SHADDA)\n",
        "\n",
        "        strip_tatweel(:obj: `bool`): remove tatweel '\\\\u0640'\n",
        "\n",
        "        insert_white_spaces(:obj: `bool`): insert whitespace before and after all non Arabic digits or English digits or Arabic and English Alphabet or the 2 brackets, then inserts whitespace between words and numbers or numbers and words\n",
        "\n",
        "        remove_elongation(:obj: `bool`): replace repetition of more than 2 non-digit character with 2 of this character\n",
        "\n",
        "        \"\"\"\n",
        "        model_name = model_name.replace(\"aubmindlab/\", \"\")\n",
        "\n",
        "        if model_name not in ACCEPTED_MODELS:\n",
        "            logging.warning(\n",
        "                \"Model provided is not in the accepted model list. Assuming you don't want Farasa Segmentation\"\n",
        "            )\n",
        "            self.model_name = \"bert-base-arabertv02\"\n",
        "        else:\n",
        "            self.model_name = model_name\n",
        "\n",
        "        if self.model_name in SEGMENTED_MODELS:\n",
        "            logging.info(\n",
        "                \"Selected Model requires pre-segmentation, Initializing FarasaSegmenter\"\n",
        "            )\n",
        "            try:\n",
        "                from farasa.segmenter import FarasaSegmenter\n",
        "\n",
        "                self.farasa_segmenter = FarasaSegmenter(interactive=True)\n",
        "            except:\n",
        "                logging.warning(\n",
        "                    \"farasapy is not installed, you want be able to process text for AraBERTv1 and v2. Install it using: pip install farasapy\"\n",
        "                )\n",
        "        else:\n",
        "            logging.info(\n",
        "                \"Selected Model doesn't require pre-segmentation, skipping FarasaSegmenter initialization\"\n",
        "            )\n",
        "\n",
        "        self.keep_emojis = keep_emojis\n",
        "        if self.keep_emojis:\n",
        "            import emoji\n",
        "\n",
        "            self.emoji = emoji\n",
        "            if self.model_name in SEGMENTED_MODELS:\n",
        "                logging.warning(\n",
        "                    \"Keeping tweets with Farasa Segmentation is 10 times slower\"\n",
        "                )\n",
        "\n",
        "        self.remove_html_markup = remove_html_markup\n",
        "        self.replace_urls_emails_mentions = replace_urls_emails_mentions\n",
        "        self.strip_tashkeel = strip_tashkeel\n",
        "        self.strip_tatweel = strip_tatweel\n",
        "        self.insert_white_spaces = insert_white_spaces\n",
        "        self.remove_elongation = remove_elongation\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess takes an input text line an applies the same preprocessing used in AraBERT\n",
        "                            pretraining\n",
        "\n",
        "        Args:\n",
        "\n",
        "            text (:obj:`str`): inout text string\n",
        "\n",
        "        Returns:\n",
        "\n",
        "            string: A preprocessed string depending on which model was selected\n",
        "        \"\"\"\n",
        "        if self.model_name == \"bert-base-arabert\":\n",
        "            return self._old_preprocess(\n",
        "                text,\n",
        "                do_farasa_tokenization=True,\n",
        "            )\n",
        "\n",
        "        if self.model_name == \"bert-base-arabertv01\":\n",
        "            return self._old_preprocess(text, do_farasa_tokenization=False)\n",
        "\n",
        "        text = str(text)\n",
        "        text = html.unescape(text)\n",
        "        if self.strip_tashkeel:\n",
        "            text = araby.strip_tashkeel(text)\n",
        "        if self.strip_tatweel:\n",
        "            text = araby.strip_tatweel(text)\n",
        "\n",
        "        if self.replace_urls_emails_mentions:\n",
        "            # replace all possible URLs\n",
        "            for reg in url_regexes:\n",
        "                text = re.sub(reg, \" [رابط] \", text)\n",
        "            # REplace Emails with [بريد]\n",
        "            for reg in email_regexes:\n",
        "                text = re.sub(reg, \" [بريد] \", text)\n",
        "            # replace mentions with [مستخدم]\n",
        "            text = re.sub(user_mention_regex, \" [مستخدم] \", text)\n",
        "\n",
        "        if self.remove_html_markup:\n",
        "            # remove html line breaks\n",
        "            text = re.sub(\"<br />\", \" \", text)\n",
        "            # remove html markup\n",
        "            text = re.sub(\"</?[^>]+>\", \" \", text)\n",
        "\n",
        "        # remove repeated characters >2\n",
        "        if self.remove_elongation:\n",
        "            text = self._remove_elongation(text)\n",
        "\n",
        "        # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets\n",
        "        if self.insert_white_spaces:\n",
        "            text = re.sub(\n",
        "                \"([^0-9\\u0621-\\u063A\\u0641-\\u064A\\u0660-\\u0669a-zA-Z\\[\\]])\",\n",
        "                r\" \\1 \",\n",
        "                text,\n",
        "            )\n",
        "\n",
        "            # insert whitespace between words and numbers or numbers and words\n",
        "            text = re.sub(\n",
        "                \"(\\d+)([\\u0621-\\u063A\\u0641-\\u064A\\u0660-\\u066C]+)\", r\" \\1 \\2 \", text\n",
        "            )\n",
        "            text = re.sub(\n",
        "                \"([\\u0621-\\u063A\\u0641-\\u064A\\u0660-\\u066C]+)(\\d+)\", r\" \\1 \\2 \", text\n",
        "            )\n",
        "\n",
        "        # remove unwanted characters\n",
        "        if self.keep_emojis:\n",
        "            emoji_regex = \"\".join(list(self.emoji.UNICODE_EMOJI[\"en\"].keys()))\n",
        "            rejected_chars_regex2 = \"[^%s%s]\" % (chars_regex, emoji_regex)\n",
        "            text = re.sub(rejected_chars_regex2, \" \", text)\n",
        "        else:\n",
        "            text = re.sub(rejected_chars_regex, \" \", text)\n",
        "\n",
        "        # remove extra spaces\n",
        "        text = \" \".join(text.replace(\"\\uFE0F\", \"\").split())\n",
        "\n",
        "        if (\n",
        "            self.model_name == \"bert-base-arabertv2\"\n",
        "            or self.model_name == \"bert-large-arabertv2\"\n",
        "        ):\n",
        "            if self.keep_emojis:\n",
        "                new_text = []\n",
        "                for word in text.split():\n",
        "                    if word in list(self.emoji.UNICODE_EMOJI[\"en\"].keys()):\n",
        "                        new_text.append(word)\n",
        "                    else:\n",
        "                        new_text.append(self.farasa_segmenter.segment(word))\n",
        "                text = \" \".join(new_text)\n",
        "            else:\n",
        "                text = self.farasa_segmenter.segment(text)\n",
        "            return self._farasa_segment(text)\n",
        "\n",
        "        # ALl the other models dont require Farasa Segmentation\n",
        "        return text\n",
        "\n",
        "    def unpreprocess(self, text, desegment=True):\n",
        "        \"\"\"Re-formats the text to a classic format where punctuations, brackets, parenthesis are not seperated by whitespaces.\n",
        "        The objective is to make the generated text of any model appear natural and not preprocessed.\n",
        "\n",
        "        Args:\n",
        "            text (str): input text to be un-preprocessed\n",
        "            desegment (bool, optional): [whether or not to remove farasa pre-segmentation before]. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            str: The unpreprocessed (and possibly Farasa-desegmented) text.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.model_name in SEGMENTED_MODELS and desegment:\n",
        "            text = self.desegment(text)\n",
        "\n",
        "        # removes the spaces around quotation marks ex: i \" ate \" an apple --> i \"ate\" an apple\n",
        "        # https://stackoverflow.com/a/53436792/5381220\n",
        "        text = re.sub(white_spaced_double_quotation_regex, '\"' + r\"\\1\" + '\"', text)\n",
        "        text = re.sub(white_spaced_single_quotation_regex, \"'\" + r\"\\1\" + \"'\", text)\n",
        "        text = re.sub(white_spaced_back_quotation_regex, \"\\`\" + r\"\\1\" + \"\\`\", text)\n",
        "        text = re.sub(white_spaced_back_quotation_regex, \"\\—\" + r\"\\1\" + \"\\—\", text)\n",
        "\n",
        "        # during generation, sometimes the models don't put a space after the dot, this handles it\n",
        "        text = text.replace(\".\", \" . \")\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        # handle decimals\n",
        "        text = re.sub(r\"(\\d+) \\. (\\d+)\", r\"\\1.\\2\", text)\n",
        "        text = re.sub(r\"(\\d+) \\, (\\d+)\", r\"\\1,\\2\", text)\n",
        "\n",
        "        text = re.sub(left_and_right_spaced_chars, r\"\\1\", text)\n",
        "        text = re.sub(left_spaced_chars, r\"\\1\", text)\n",
        "        text = re.sub(right_spaced_chars, r\"\\1\", text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def desegment(self, text):\n",
        "        \"\"\"\n",
        "        Use this function if sentence tokenization was done using\n",
        "        `from arabert.preprocess_arabert import preprocess` with Farasa enabled\n",
        "        AraBERT segmentation using Farasa adds a space after the '+' for prefixes,\n",
        "        and after before the '+' for suffixes\n",
        "\n",
        "        Example:\n",
        "        >>> desegment('ال+ دراس +ات')\n",
        "        الدراسات\n",
        "        \"\"\"\n",
        "        text = text.replace(\"+ \", \"+\")\n",
        "        text = text.replace(\" +\", \"+\")\n",
        "        text = \" \".join([self._desegmentword(word) for word in text.split(\" \")])\n",
        "        return text\n",
        "\n",
        "    def _desegmentword(self, orig_word: str) -> str:\n",
        "        \"\"\"\n",
        "        Word segmentor that takes a Farasa Segmented Word and removes the '+' signs\n",
        "\n",
        "        Example:\n",
        "        >>> _desegmentword(\"ال+يومي+ة\")\n",
        "        اليومية\n",
        "        \"\"\"\n",
        "        word = orig_word.replace(\"ل+ال+\", \"لل\")\n",
        "        if \"ال+ال\" not in orig_word:\n",
        "            word = word.replace(\"ل+ال\", \"لل\")\n",
        "        word = word.replace(\"+\", \"\")\n",
        "        word = word.replace(\"للل\", \"لل\")\n",
        "        return word\n",
        "\n",
        "    def _old_preprocess(self, text, do_farasa_tokenization):\n",
        "        \"\"\"\n",
        "        AraBERTv1 preprocessing Function\n",
        "        \"\"\"\n",
        "        text = str(text)\n",
        "        if self.strip_tashkeel:\n",
        "            text = araby.strip_tashkeel(text)\n",
        "\n",
        "        text = re.sub(r\"\\d+\\/[ء-ي]+\\/\\d+\\]\", \"\", text)\n",
        "        text = re.sub(\"ـ\", \"\", text)\n",
        "        text = re.sub(\"[«»]\", ' \" ', text)\n",
        "\n",
        "        if self.replace_urls_emails_mentions:\n",
        "            # replace the [رابط] token with space if you want to clean links\n",
        "            text = re.sub(regex_url_step1, \"[رابط]\", text)\n",
        "            text = re.sub(regex_url_step2, \"[رابط]\", text)\n",
        "            text = re.sub(regex_url, \"[رابط]\", text)\n",
        "            text = re.sub(regex_email, \"[بريد]\", text)\n",
        "            text = re.sub(regex_mention, \"[مستخدم]\", text)\n",
        "        text = re.sub(\"…\", r\"\\.\", text).strip()\n",
        "        text = self._remove_redundant_punct(text)\n",
        "\n",
        "        if self.replace_urls_emails_mentions:\n",
        "            text = re.sub(r\"\\[ رابط \\]|\\[ رابط\\]|\\[رابط \\]\", \" [رابط] \", text)\n",
        "            text = re.sub(r\"\\[ بريد \\]|\\[ بريد\\]|\\[بريد \\]\", \" [بريد] \", text)\n",
        "            text = re.sub(r\"\\[ مستخدم \\]|\\[ مستخدم\\]|\\[مستخدم \\]\", \" [مستخدم] \", text)\n",
        "\n",
        "        if self.remove_elongation:\n",
        "            text = self._remove_elongation(text)\n",
        "\n",
        "        if self.insert_white_spaces:\n",
        "            text = re.sub(\n",
        "                \"([^0-9\\u0621-\\u063A\\u0641-\\u0669\\u0671-\\u0673a-zA-Z\\[\\]])\",\n",
        "                r\" \\1 \",\n",
        "                text,\n",
        "            )\n",
        "        if do_farasa_tokenization:\n",
        "            text = self._tokenize_arabic_words_farasa(text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _farasa_segment(self, text):\n",
        "        line_farasa = text.split()\n",
        "        segmented_line = []\n",
        "        for index, word in enumerate(line_farasa):\n",
        "            if word in [\"[\", \"]\"]:\n",
        "                continue\n",
        "            if word in [\"رابط\", \"بريد\", \"مستخدم\"] and line_farasa[index - 1] in [\n",
        "                \"[\",\n",
        "                \"]\",\n",
        "            ]:\n",
        "                segmented_line.append(\"[\" + word + \"]\")\n",
        "                continue\n",
        "            if \"+\" not in word:\n",
        "                segmented_line.append(word)\n",
        "                continue\n",
        "            segmented_word = self._split_farasa_output(word)\n",
        "            segmented_line.extend(segmented_word)\n",
        "\n",
        "        return \" \".join(segmented_line)\n",
        "\n",
        "    def _split_farasa_output(self, word):\n",
        "        segmented_word = []\n",
        "        temp_token = \"\"\n",
        "        for i, c in enumerate(word):\n",
        "            if c == \"+\":\n",
        "                # if the token is KAF, it could be a suffix or prefix\n",
        "                if temp_token == \"ك\":\n",
        "                    # if we are at the second token, then KAF is surely a prefix\n",
        "                    if i == 1:\n",
        "                        segmented_word.append(temp_token + \"+\")\n",
        "                        temp_token = \"\"\n",
        "                    # If the KAF token is between 2 tokens\n",
        "                    elif word[i - 2] == \"+\":\n",
        "                        # if the previous token is prefix, then this KAF must be a prefix\n",
        "                        if segmented_word[-1][-1] == \"+\":\n",
        "                            segmented_word.append(temp_token + \"+\")\n",
        "                            temp_token = \"\"\n",
        "                        # else it is a suffix, this KAF could not be a second suffix\n",
        "                        else:\n",
        "                            segmented_word.append(\"+\" + temp_token)\n",
        "                            temp_token = \"\"\n",
        "                    # if Kaf is at the end, this is handled with the statement after the loop\n",
        "                elif temp_token in prefix_list:\n",
        "                    segmented_word.append(temp_token + \"+\")\n",
        "                    temp_token = \"\"\n",
        "                elif temp_token in suffix_list:\n",
        "                    segmented_word.append(\"+\" + temp_token)\n",
        "                    temp_token = \"\"\n",
        "                else:\n",
        "                    segmented_word.append(temp_token)\n",
        "                    temp_token = \"\"\n",
        "                continue\n",
        "            temp_token += c\n",
        "        if temp_token != \"\":\n",
        "            if temp_token in suffix_list:\n",
        "                segmented_word.append(\"+\" + temp_token)\n",
        "            else:\n",
        "                segmented_word.append(temp_token)\n",
        "        return segmented_word\n",
        "\n",
        "    def _tokenize_arabic_words_farasa(self, line_input):\n",
        "\n",
        "        if self.keep_emojis:\n",
        "            # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets\n",
        "            line_farasa = []\n",
        "            for word in line_input.split():\n",
        "                if word in list(self.emoji.UNICODE_EMOJI[\"en\"].keys()):\n",
        "                    line_farasa.append(word)\n",
        "                else:\n",
        "                    line_farasa.append(self.farasa_segmenter.segment(word))\n",
        "        else:\n",
        "            line_farasa = self.farasa_segmenter.segment(line_input).split()\n",
        "\n",
        "        segmented_line = []\n",
        "        for index, word in enumerate(line_farasa):\n",
        "            if word in [\"[\", \"]\"]:\n",
        "                continue\n",
        "            if word in [\"رابط\", \"بريد\", \"مستخدم\"] and line_farasa[index - 1] in [\n",
        "                \"[\",\n",
        "                \"]\",\n",
        "            ]:\n",
        "                segmented_line.append(\"[\" + word + \"]\")\n",
        "                continue\n",
        "            segmented_word = []\n",
        "            for token in word.split(\"+\"):\n",
        "                if token in prefix_list:\n",
        "                    segmented_word.append(token + \"+\")\n",
        "                elif token in suffix_list:\n",
        "                    segmented_word.append(\"+\" + token)\n",
        "                else:\n",
        "                    segmented_word.append(token)\n",
        "            segmented_line.extend(segmented_word)\n",
        "        return \" \".join(segmented_line)\n",
        "\n",
        "    def _remove_elongation(self, text):\n",
        "        \"\"\"\n",
        "        :param text:  the input text to remove elongation\n",
        "        :return: delongated text\n",
        "        \"\"\"\n",
        "        # loop over the number of times the regex matched the text\n",
        "        for index_ in range(len(re.findall(regex_tatweel, text))):\n",
        "            elongation = re.search(regex_tatweel, text)\n",
        "            if elongation:\n",
        "                elongation_pattern = elongation.group()\n",
        "                elongation_replacement = elongation_pattern[0]\n",
        "                elongation_pattern = re.escape(elongation_pattern)\n",
        "                text = re.sub(\n",
        "                    elongation_pattern, elongation_replacement, text, flags=re.MULTILINE\n",
        "                )\n",
        "            else:\n",
        "                break\n",
        "        return text\n",
        "\n",
        "    def _remove_redundant_punct(self, text):\n",
        "        text_ = text\n",
        "        result = re.search(redundant_punct_pattern, text)\n",
        "        dif = 0\n",
        "        while result:\n",
        "            sub = result.group()\n",
        "            sub = sorted(set(sub), key=sub.index)\n",
        "            sub = \" \" + \"\".join(list(sub)) + \" \"\n",
        "            text = \"\".join(\n",
        "                (text[: result.span()[0] + dif], sub, text[result.span()[1] + dif :])\n",
        "            )\n",
        "            text_ = \"\".join(\n",
        "                (text_[: result.span()[0]], text_[result.span()[1] :])\n",
        "            ).strip()\n",
        "            dif = abs(len(text) - len(text_))\n",
        "            result = re.search(redundant_punct_pattern, text_)\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "        return text.strip()\n",
        "\n",
        "\n",
        "prefix_list = [\n",
        "    \"ال\",\n",
        "    \"و\",\n",
        "    \"ف\",\n",
        "    \"ب\",\n",
        "    \"ك\",\n",
        "    \"ل\",\n",
        "    \"لل\",\n",
        "    \"\\u0627\\u0644\",\n",
        "    \"\\u0648\",\n",
        "    \"\\u0641\",\n",
        "    \"\\u0628\",\n",
        "    \"\\u0643\",\n",
        "    \"\\u0644\",\n",
        "    \"\\u0644\\u0644\",\n",
        "    \"س\",\n",
        "]\n",
        "suffix_list = [\n",
        "    \"ه\",\n",
        "    \"ها\",\n",
        "    \"ك\",\n",
        "    \"ي\",\n",
        "    \"هما\",\n",
        "    \"كما\",\n",
        "    \"نا\",\n",
        "    \"كم\",\n",
        "    \"هم\",\n",
        "    \"هن\",\n",
        "    \"كن\",\n",
        "    \"ا\",\n",
        "    \"ان\",\n",
        "    \"ين\",\n",
        "    \"ون\",\n",
        "    \"وا\",\n",
        "    \"ات\",\n",
        "    \"ت\",\n",
        "    \"ن\",\n",
        "    \"ة\",\n",
        "    \"\\u0647\",\n",
        "    \"\\u0647\\u0627\",\n",
        "    \"\\u0643\",\n",
        "    \"\\u064a\",\n",
        "    \"\\u0647\\u0645\\u0627\",\n",
        "    \"\\u0643\\u0645\\u0627\",\n",
        "    \"\\u0646\\u0627\",\n",
        "    \"\\u0643\\u0645\",\n",
        "    \"\\u0647\\u0645\",\n",
        "    \"\\u0647\\u0646\",\n",
        "    \"\\u0643\\u0646\",\n",
        "    \"\\u0627\",\n",
        "    \"\\u0627\\u0646\",\n",
        "    \"\\u064a\\u0646\",\n",
        "    \"\\u0648\\u0646\",\n",
        "    \"\\u0648\\u0627\",\n",
        "    \"\\u0627\\u062a\",\n",
        "    \"\\u062a\",\n",
        "    \"\\u0646\",\n",
        "    \"\\u0629\",\n",
        "]\n",
        "other_tokens = [\"[رابط]\", \"[مستخدم]\", \"[بريد]\"]\n",
        "\n",
        "# the never_split list is ussed with the transformers library\n",
        "prefix_symbols = [x + \"+\" for x in prefix_list]\n",
        "suffix_symblos = [\"+\" + x for x in suffix_list]\n",
        "never_split_tokens = list(set(prefix_symbols + suffix_symblos + other_tokens))\n",
        "\n",
        "url_regexes = [\n",
        "    r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\",\n",
        "    r\"@(https?|ftp)://(-\\.)?([^\\s/?\\.#-]+\\.?)+(/[^\\s]*)?$@iS\",\n",
        "    r\"http[s]?://[a-zA-Z0-9_\\-./~\\?=%&]+\",\n",
        "    r\"www[a-zA-Z0-9_\\-?=%&/.~]+\",\n",
        "    r\"[a-zA-Z]+\\.com\",\n",
        "    r\"(?=http)[^\\s]+\",\n",
        "    r\"(?=www)[^\\s]+\",\n",
        "    r\"://\",\n",
        "]\n",
        "user_mention_regex = r\"@[\\w\\d]+\"\n",
        "email_regexes = [r\"[\\w-]+@([\\w-]+\\.)+[\\w-]+\", r\"\\S+@\\S+\"]\n",
        "redundant_punct_pattern = (\n",
        "    r\"([!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ【»؛\\s+«–…‘]{2,})\"\n",
        ")\n",
        "regex_tatweel = r\"(\\D)\\1{2,}\"\n",
        "rejected_chars_regex = r\"[^0-9\\u0621-\\u063A\\u0640-\\u066C\\u0671-\\u0674a-zA-Z\\[\\]!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ»؛\\s+«–…‘]\"\n",
        "\n",
        "regex_url_step1 = r\"(?=http)[^\\s]+\"\n",
        "regex_url_step2 = r\"(?=www)[^\\s]+\"\n",
        "regex_url = r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
        "regex_mention = r\"@[\\w\\d]+\"\n",
        "regex_email = r\"\\S+@\\S+\"\n",
        "\n",
        "chars_regex = r\"0-9\\u0621-\\u063A\\u0640-\\u066C\\u0671-\\u0674a-zA-Z\\[\\]!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ»؛\\s+«–…‘\"\n",
        "\n",
        "white_spaced_double_quotation_regex = r'\\\"\\s+([^\"]+)\\s+\\\"'\n",
        "white_spaced_single_quotation_regex = r\"\\'\\s+([^']+)\\s+\\'\"\n",
        "white_spaced_back_quotation_regex = r\"\\`\\s+([^`]+)\\s+\\`\"\n",
        "white_spaced_em_dash = r\"\\—\\s+([^—]+)\\s+\\—\"\n",
        "\n",
        "left_spaced_chars = r\" ([\\]!#\\$%\\),\\.:;\\?}٪’،؟”؛…»·])\"\n",
        "right_spaced_chars = r\"([\\[\\(\\{“«‘*\\~]) \"\n",
        "left_and_right_spaced_chars = r\" ([\\+\\-\\<\\=\\>\\@\\\\\\^\\_\\|\\–]) \"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
